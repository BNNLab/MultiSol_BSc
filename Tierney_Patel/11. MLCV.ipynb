{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e324e0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.2.5\n",
      "Uninstalling numpy-2.2.5:\n",
      "  Successfully uninstalled numpy-2.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.5-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.2.5-cp312-cp312-macosx_14_0_x86_64.whl (6.7 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.5 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
      "gpy 1.13.2 requires numpy<2.0.0,>=1.7, but you have numpy 2.2.5 which is incompatible.\n",
      "scipy 1.12.0 requires numpy<1.29.0,>=1.22.4, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall numpy -y\n",
    "%pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c074f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x111a23c20>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/numpy/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy<2\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"numpy<2\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5053426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:43: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/m2/ptj22ckn1lq9dwc6k6bwvz580000gn/T/ipykernel_43069/1911255660.py:43: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  x=re.findall('\\d\\.\\d+',str(R2[i]))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This python script performs 10-fold cross validation for a dataset for 7 ML methods and outputs:\n",
    "1. The resulting average metrics\n",
    "2. Predicted vs experimental values for all folds and models\n",
    "'''\n",
    "#section 1: import modules\n",
    "import sys,os,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn import ensemble\n",
    "from scipy.stats import pearsonr\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics\n",
    "\n",
    "##section 2: define inputs and outputs\n",
    "dir = os.getcwd() \n",
    "Dataset=pd.read_csv(os.path.join(dir,\"/Users/stella/Downloads/tierney/project/all_final_data.csv\")) #location of descriptor file\n",
    "output_metrics=os.path.join(dir,\"/Users/stella/Downloads/tierney/project/MLCV_metrics.csv\") #location of output file for metrics\n",
    "output_predictions=os.path.join(dir,\"/Users/stella/Downloads/tierney/project/MLCV_LogS_predictions.csv\") #new file for predictions\n",
    "\n",
    "##section 3: define methods\n",
    "#Define statistical measures and R2 conversion\n",
    "#define RMSE\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "#define % within certain range\n",
    "def within_range(list1, list2, range2):\n",
    "    x=0\n",
    "    for i in range(len(list2)):\n",
    "        if (list1[i]-range2)<= list2[i] <= (list1[i]+range2): \n",
    "            x+=1\n",
    "    return((float(x)/(len(list2)))*100)\n",
    "#define getting R2 method\n",
    "def get_R2(R2):\n",
    "    R2_2=[]\n",
    "    for i in range(len(R2)):\n",
    "        x=re.findall('\\d\\.\\d+',str(R2[i]))\n",
    "        j=float(x[0])\n",
    "        j=j**2\n",
    "        R2_2.append(j)\n",
    "    return(R2_2)\n",
    "#define method to get CV results\n",
    "def CV_metrics(Data,folds,C,E,G):\n",
    "    #initiate lists to add metrics to\n",
    "    RMSE=[]\n",
    "    R2=[]\n",
    "    N1=[]\n",
    "    N05=[]\n",
    "    MLR_RMSE=[]\n",
    "    MLR_R2=[]\n",
    "    MLR_N1=[]\n",
    "    MLR_N05=[]\n",
    "    ANN_RMSE=[]\n",
    "    ANN_R2=[]\n",
    "    ANN_N1=[]\n",
    "    ANN_N05=[]\n",
    "    SVM_RMSE=[]\n",
    "    SVM_R2=[]\n",
    "    SVM_N1=[]\n",
    "    SVM_N05=[]\n",
    "    PLS_RMSE=[]\n",
    "    PLS_R2=[]\n",
    "    PLS_N1=[]\n",
    "    PLS_N05=[]\n",
    "    RF_RMSE=[]\n",
    "    RF_R2=[]\n",
    "    RF_N1=[]\n",
    "    RF_N05=[]\n",
    "    ET_RMSE=[]\n",
    "    ET_R2=[]\n",
    "    ET_N1=[]\n",
    "    ET_N05=[]\n",
    "    BG_RMSE=[]\n",
    "    BG_R2=[]\n",
    "    BG_N1=[]\n",
    "    BG_N05=[]\n",
    "    \n",
    "    # Initialize lists to store predictions and experimental values\n",
    "    all_preds = []\n",
    "    \n",
    "    #import Data and randomise\n",
    "    X = Data\n",
    "    X = X.sample(frac=1).reset_index(drop=True)\n",
    "    #define k-fold cross validation and make k splits\n",
    "    col_names=X.dtypes.index\n",
    "    X = np.array(X)\n",
    "    kf = KFold(n_splits=folds)\n",
    "    fold_num = 1\n",
    "    \n",
    "    #for every split\n",
    "    for train1, test1 in kf.split(X):\n",
    "        train=X[train1]\n",
    "        test=X[test1]\n",
    "        train=pd.DataFrame(data=train, columns=col_names)\n",
    "        test=pd.DataFrame(data=test, columns=col_names)\n",
    "        \n",
    "        # Get compound identifiers and solvent information\n",
    "        inchikeys = test['InChIkey'] if 'InChIkey' in test.columns else ['Unknown']*len(test)\n",
    "        solvents = test['Solvent'] if 'Solvent' in test.columns else ['Unknown']*len(test)\n",
    "        \n",
    "        X_train = train[['MW', 'Volume', 'G_sol', 'DeltaG_sol', 'sol_dip',\n",
    "                         'LsoluHsolv', 'LsolvHsolu', 'SASA', 'O_charges',\n",
    "                         'C_charges', 'Most_neg', 'Most_pos', 'Het_charges']]\n",
    "        y_train = train['LogS']\n",
    "        X_test = test[['MW', 'Volume', 'G_sol', 'DeltaG_sol', 'sol_dip',\n",
    "                       'LsoluHsolv', 'LsolvHsolu', 'SASA', 'O_charges',\n",
    "                       'C_charges', 'Most_neg', 'Most_pos', 'Het_charges']]\n",
    "        y_test = test['LogS']\n",
    "        \n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "        # Create dictionary to store this fold's predictions\n",
    "        fold_preds = {\n",
    "            'Fold': [fold_num] * len(y_test),\n",
    "            'InChIkey': inchikeys,\n",
    "            'Solvent': solvents,\n",
    "            'Experimental': y_test,\n",
    "        }\n",
    "        \n",
    "        #run models\n",
    "        #MLR\n",
    "        mlr = LinearRegression()\n",
    "        mlr.fit(X_train, y_train)\n",
    "        mlr2preds = mlr.predict(X_test)\n",
    "        fold_preds['MLR'] = mlr2preds\n",
    "        #evaluate model\n",
    "        MLR_R2.append(pearsonr(mlr2preds, y_test))\n",
    "        MLR_RMSE.append(rmse(mlr2preds, y_test))\n",
    "        MLR_N1.append(within_range(y_test,mlr2preds,1))\n",
    "        MLR_N05.append(within_range(y_test,mlr2preds,0.7))\n",
    "        \n",
    "        #ANN\n",
    "        mlp = MLPRegressor(hidden_layer_sizes=300,max_iter=800)\n",
    "        for f in range(100):\n",
    "            try:\n",
    "                mlp.fit(X_train, y_train)\n",
    "                mlp2preds = mlp.predict(X_test)\n",
    "                if np.ptp(mlp2preds) == 0:\n",
    "                    continue\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        fold_preds['ANN'] = mlp2preds\n",
    "        #evaluate model\n",
    "        ANN_R2.append(pearsonr(mlp2preds, y_test))\n",
    "        ANN_RMSE.append(rmse(mlp2preds, y_test))\n",
    "        ANN_N1.append(within_range(y_test,mlp2preds,1))\n",
    "        ANN_N05.append(within_range(y_test,mlp2preds,0.7))\n",
    "        \n",
    "        #SVM\n",
    "        svm2 = svm.SVR(C = C, epsilon = E, gamma = G, kernel = 'rbf')\n",
    "        svm2.fit(X_train, y_train)\n",
    "        svm2preds = svm2.predict(X_test)\n",
    "        fold_preds['SVM'] = svm2preds\n",
    "        #evaluate model\n",
    "        SVM_R2.append(pearsonr(svm2preds, y_test))\n",
    "        SVM_RMSE.append(rmse(svm2preds, y_test))\n",
    "        SVM_N1.append(within_range(y_test,svm2preds,1))\n",
    "        SVM_N05.append(within_range(y_test,svm2preds,0.7))\n",
    "        \n",
    "        #PLS\n",
    "        pls2 = PLSRegression(n_components=9)\n",
    "        pls2.fit(X_train, y_train)\n",
    "        pls2preds = pls2.predict(X_test)\n",
    "        #convert to float (comes in weird type?)\n",
    "        pls2preds2=[]\n",
    "        for i in pls2preds:\n",
    "            pls2preds2.append(float(i))\n",
    "        fold_preds['PLS'] = pls2preds2\n",
    "        #evaluate model\n",
    "        PLS_R2.append(pearsonr(pls2preds2, y_test))\n",
    "        PLS_RMSE.append(rmse(pls2preds2, y_test))\n",
    "        PLS_N1.append(within_range(y_test,pls2preds2,1))\n",
    "        PLS_N05.append(within_range(y_test,pls2preds2,0.7))\n",
    "        \n",
    "        #RF\n",
    "        tree2 = ensemble.RandomForestRegressor(n_estimators=500, n_jobs=1)\n",
    "        tree2.fit(X_train, y_train)\n",
    "        tree2preds = tree2.predict(X_test)\n",
    "        fold_preds['RF'] = tree2preds\n",
    "        #evaluate model\n",
    "        RF_R2.append(pearsonr(tree2preds, y_test))\n",
    "        RF_RMSE.append(rmse(tree2preds, y_test))\n",
    "        RF_N1.append(within_range(y_test, tree2preds, 1))\n",
    "        RF_N05.append(within_range(y_test, tree2preds, 0.7))\n",
    "        \n",
    "        #ExtraTrees\n",
    "        tree3 = ensemble.ExtraTreesRegressor(n_estimators=500, n_jobs=1)\n",
    "        tree3.fit(X_train, y_train)\n",
    "        tree3preds = tree3.predict(X_test)\n",
    "        fold_preds['ExtraTrees'] = tree3preds\n",
    "        #evaluate model\n",
    "        ET_R2.append(pearsonr(tree3preds, y_test))\n",
    "        ET_RMSE.append(rmse(tree3preds, y_test))\n",
    "        ET_N1.append(within_range(y_test, tree3preds, 1))\n",
    "        ET_N05.append(within_range(y_test, tree3preds, 0.7))\n",
    "        \n",
    "        #Bagging\n",
    "        tree4 = ensemble.BaggingRegressor(n_estimators=500, n_jobs=1)\n",
    "        tree4.fit(X_train, y_train)\n",
    "        tree4preds = tree4.predict(X_test)\n",
    "        fold_preds['Bagging'] = tree4preds\n",
    "        #evaluate model\n",
    "        BG_R2.append(pearsonr(tree4preds, y_test))\n",
    "        BG_RMSE.append(rmse(tree4preds, y_test))\n",
    "        BG_N1.append(within_range(y_test, tree4preds, 1))\n",
    "        BG_N05.append(within_range(y_test, tree4preds, 0.7))\n",
    "        \n",
    "        # Add this fold's predictions to the main list\n",
    "        all_preds.append(pd.DataFrame(fold_preds))\n",
    "        fold_num += 1\n",
    "    \n",
    "    # Combine all fold predictions into one DataFrame\n",
    "    all_preds_df = pd.concat(all_preds, ignore_index=True)\n",
    "    \n",
    "    #get R2 from Pearson output\n",
    "    MLR_R2=get_R2(MLR_R2)\n",
    "    ANN_R2=get_R2(ANN_R2)\n",
    "    SVM_R2=get_R2(SVM_R2)\n",
    "    PLS_R2=get_R2(PLS_R2)\n",
    "    RF_R2=get_R2(RF_R2)\n",
    "    ET_R2=get_R2(ET_R2)\n",
    "    BG_R2=get_R2(BG_R2)\n",
    "    \n",
    "    #get mean metrics and put together in lists\n",
    "    R2.append(statistics.mean(MLR_R2))\n",
    "    RMSE.append(statistics.mean(MLR_RMSE))\n",
    "    N1.append(statistics.mean(MLR_N1))\n",
    "    N05.append(statistics.mean(MLR_N05))\n",
    "    \n",
    "    R2.append(statistics.mean(ANN_R2))\n",
    "    RMSE.append(statistics.mean(ANN_RMSE))\n",
    "    N1.append(statistics.mean(ANN_N1))\n",
    "    N05.append(statistics.mean(ANN_N05))\n",
    "    \n",
    "    R2.append(statistics.mean(SVM_R2))\n",
    "    RMSE.append(statistics.mean(SVM_RMSE))\n",
    "    N1.append(statistics.mean(SVM_N1))\n",
    "    N05.append(statistics.mean(SVM_N05))\n",
    "    \n",
    "    R2.append(statistics.mean(PLS_R2))\n",
    "    RMSE.append(statistics.mean(PLS_RMSE))\n",
    "    N1.append(statistics.mean(PLS_N1))\n",
    "    N05.append(statistics.mean(PLS_N05))\n",
    "    \n",
    "    R2.append(statistics.mean(RF_R2))\n",
    "    RMSE.append(statistics.mean(RF_RMSE))\n",
    "    N1.append(statistics.mean(RF_N1))\n",
    "    N05.append(statistics.mean(RF_N05))\n",
    "    \n",
    "    R2.append(statistics.mean(ET_R2))\n",
    "    RMSE.append(statistics.mean(ET_RMSE))\n",
    "    N1.append(statistics.mean(ET_N1))\n",
    "    N05.append(statistics.mean(ET_N05))\n",
    "    \n",
    "    R2.append(statistics.mean(BG_R2))\n",
    "    RMSE.append(statistics.mean(BG_RMSE))\n",
    "    N1.append(statistics.mean(BG_N1))\n",
    "    N05.append(statistics.mean(BG_N05))\n",
    "    \n",
    "    #create dataframe of metrics\n",
    "    Models=[\"MLR\",\"ANN\",\"SVM\",\"PLS\",\"RF\",\"ExtraTrees\",\"Bagging\"]\n",
    "    Metrics=list(zip(Models,R2,RMSE,N1,N05))\n",
    "    Metrics_df=pd.DataFrame(data=Metrics, columns=['Model','R2','RMSE','% within 1','% within 0.7'])\n",
    "    \n",
    "    return Metrics_df, all_preds_df\n",
    "\n",
    "##method to put it all together\n",
    "def get_CV_metrics(Dataset,output_metrics,output_predictions):\n",
    "    ##get metrics and predictions\n",
    "    CV_metrics2, all_predictions = CV_metrics(Dataset,10,4,0.01,0.03) ##10-folds and SVM parameters\n",
    "    \n",
    "    ##save to files\n",
    "    CV_metrics2.to_csv(output_metrics,index=False)\n",
    "    all_predictions.to_csv(output_predictions,index=False)\n",
    "\n",
    "##section 4: run CV method and get metrics and predictions\n",
    "get_CV_metrics(Dataset,output_metrics,output_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e982db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 3249 ML rows with GPR predictions\n",
      "Final dataset has 3249 rows\n",
      "\n",
      "Saved to /Users/stella/Downloads/tierney/project/all_predictions.csv\n",
      "Final columns: ['Fold', 'InChIkey', 'Solvent', 'Experimental', 'MLR', 'ANN', 'SVM', 'PLS', 'RF', 'ExtraTrees', 'Bagging', 'Predicted']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "gpr_file = \"/Users/stella/Downloads/tierney/project/GPCV_predictions.csv\"\n",
    "ml_file = \"/Users/stella/Downloads/tierney/project/MLCV_LogS_predictions.csv\"\n",
    "output_file = \"/Users/stella/Downloads/tierney/project/all_predictions.csv\"\n",
    "\n",
    "# Read files with clean column names\n",
    "gpr_preds = pd.read_csv(gpr_file)\n",
    "ml_preds = pd.read_csv(ml_file)\n",
    "\n",
    "# Clean column names\n",
    "gpr_preds.columns = gpr_preds.columns.str.strip()\n",
    "ml_preds.columns = ml_preds.columns.str.strip()\n",
    "\n",
    "# Verify required columns exist\n",
    "required = ['InChIkey', 'Solvent']\n",
    "for col in required:\n",
    "    if col not in gpr_preds.columns:\n",
    "        raise ValueError(f\"'{col}' missing in GPR file\")\n",
    "    if col not in ml_preds.columns:\n",
    "        raise ValueError(f\"'{col}' missing in ML file\")\n",
    "\n",
    "# Perform merge\n",
    "\n",
    "combined = pd.merge(\n",
    "    ml_preds,\n",
    "    gpr_preds[['InChIkey','Solvent','Predicted']],\n",
    "    on=['InChIkey','Solvent'],  # All 3 columns\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check results\n",
    "print(f\"Merged {len(ml_preds)} ML rows with GPR predictions\")\n",
    "print(f\"Final dataset has {len(combined)} rows\")\n",
    "missing = combined['Predicted'].isna().sum()\n",
    "if missing > 0:\n",
    "    print(f\"Warning: {missing} rows missing GPR predictions\")\n",
    "\n",
    "# Save output\n",
    "combined.to_csv(output_file, index=False)\n",
    "print(f\"\\nSaved to {output_file}\")\n",
    "print(\"Final columns:\", combined.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
