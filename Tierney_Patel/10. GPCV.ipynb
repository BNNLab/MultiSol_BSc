{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/stella/Downloads/tierney/project/.venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/stella/Downloads/tierney/project/.venv/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/stella/Downloads/tierney/project/.venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/stella/Downloads/tierney/project/.venv/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/stella/Downloads/tierney/project/.venv/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/stella/Downloads/tierney/project/.venv/lib/python3.9/site-packages (25.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/stella/Downloads/tierney/project/.venv/lib/python3.9/site-packages (75.6.0)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.6.0\n",
      "    Uninstalling setuptools-75.6.0:\n",
      "      Successfully uninstalled setuptools-75.6.0\n",
      "Successfully installed setuptools-78.1.0 wheel-0.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/stella/Downloads/tierney/project/.venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/stella/Downloads/tierney/project/.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/stella/Downloads/tierney/project/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp39-cp39-macosx_10_9_x86_64.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gpy\n",
      "  Downloading GPy-1.13.2-cp312-cp312-macosx_10_9_universal2.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.7 in /opt/anaconda3/lib/python3.12/site-packages (from gpy) (1.26.4)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from gpy) (1.16.0)\n",
      "Collecting paramz>=0.9.6 (from gpy)\n",
      "  Using cached paramz-0.9.6-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting cython>=0.29 (from gpy)\n",
      "  Downloading Cython-3.0.12-cp312-cp312-macosx_10_9_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting scipy<=1.12.0,>=1.3.0 (from gpy)\n",
      "  Downloading scipy-1.12.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from paramz>=0.9.6->gpy) (5.1.1)\n",
      "Downloading GPy-1.13.2-cp312-cp312-macosx_10_9_universal2.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Cython-3.0.12-cp312-cp312-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached paramz-0.9.6-py3-none-any.whl (103 kB)\n",
      "Downloading scipy-1.12.0-cp312-cp312-macosx_10_9_x86_64.whl (38.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy, cython, paramz, gpy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.1\n",
      "    Uninstalling scipy-1.13.1:\n",
      "      Successfully uninstalled scipy-1.13.1\n",
      "Successfully installed cython-3.0.12 gpy-1.13.2 paramz-0.9.6 scipy-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " <>:56: SyntaxWarning:invalid escape sequence '\\d'\n",
      " <>:56: SyntaxWarning:invalid escape sequence '\\d'\n",
      " /var/folders/m2/ptj22ckn1lq9dwc6k6bwvz580000gn/T/ipykernel_33347/2968771801.py:56: SyntaxWarning:invalid escape sequence '\\d'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This python script performs 10-fold cross validation for a dataset for GP machine learning method and outputs the resulting average metrics.\n",
    "The metrics are calculated for each of the 10 folds then the mean taken as the final metrics.\n",
    "In addition to the predictions, an upper and lower prediction (or error) is also calculated based on the prediction that encompasses 1 SD.\n",
    "INPUTS:\n",
    "Dataset.csv - a .csv file with the data in. The columns must be named in the same way as below\n",
    "OUTPUTS:\n",
    "GPCV_metrics - a .csv file of the metrics for GP method for its performance using 10-fold CV. \"Max % within\" refers to whether the prediction, with the upper and lower levels, fall within the range.\n",
    "GPCV_predictions - a .csv file containing the actual and predicted LogS values for each compound\n",
    "'''\n",
    "\n",
    "#section 1: import modules\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics\n",
    "import sys,os,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import GPy\n",
    "from scipy.stats import pearsonr\n",
    "import math\n",
    "\n",
    "##section 2: define inputs and outputs\n",
    "dir = os.getcwd() \n",
    "Dataset=pd.read_csv(os.path.join(dir,\"/Users/stella/Downloads/tierney/project/all_data_final.csv\"))\n",
    "output_metrics=os.path.join(dir,\"/Users/stella/Downloads/tierney/project/GPCV_metrics.csv\")\n",
    "output_predictions=os.path.join(dir,\"/Users/stella/Downloads/tierney/project/GPCV_predictions.csv\")\n",
    "\n",
    "##section 3: define methods\n",
    "#Define statistical measures and R2 conversion\n",
    "#define RMSE\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "#define method to find predictions within certain range\n",
    "def within_range(list1, list2, range2):\n",
    "    x=0\n",
    "    for i in range(len(list2)):\n",
    "        if (list1[i]-range2)<= list2[i] <= (list1[i]+range2): \n",
    "            x+=1\n",
    "    return((float(x)/(len(list2)))*100)\n",
    "def within_range_errors(list1, list2, list3, list4, range2):\n",
    "    x=0\n",
    "    for i in range(len(list2)):\n",
    "        if (list1[i]-range2)<= list2[i] <= (list1[i]+range2): \n",
    "            x+=1\n",
    "        elif (list1[i]-range2)<= list3[i] <= (list1[i]+range2): \n",
    "            x+=1\n",
    "        elif (list1[i]-range2)<= list4[i] <= (list1[i]+range2): \n",
    "            x+=1\n",
    "    return((float(x)/(len(list2)))*100)\n",
    "#define getting R2 method\n",
    "def get_R2(R2):\n",
    "    R2_2=[]\n",
    "    for i in range(len(R2)):\n",
    "        x=re.findall('\\d\\.\\d+',str(R2[i]))\n",
    "        j=float(x[0])\n",
    "        j=j**2\n",
    "        R2_2.append(j)\n",
    "    return(R2_2)\n",
    "#define method to get CV results\n",
    "def CV_metrics(Data,folds):\n",
    "    #initiate lists to add metrics to\n",
    "    RMSE=[]\n",
    "    R2=[]\n",
    "    N1=[]\n",
    "    N05=[]\n",
    "    N1_e=[]\n",
    "    N05_e=[]\n",
    "    GPR_RMSE=[]\n",
    "    GPR_R2=[]\n",
    "    GPR_N1=[]\n",
    "    GPR_N05=[]\n",
    "    GPR_N1_e=[]\n",
    "    GPR_N05_e=[]\n",
    "    \n",
    "    # Initialize list to store predictions and actual values\n",
    "    all_predictions = []\n",
    "    \n",
    "    #import Data\n",
    "    X = Data\n",
    "    X = X.sample(frac=1).reset_index(drop=True)\n",
    "    # Store original indices to identify compounds later\n",
    "    X['original_index'] = X.index\n",
    "    #define k-fold cross validation\n",
    "    col_names=X.dtypes.index\n",
    "    X = np.array(X)\n",
    "    kf = KFold(n_splits=folds)\n",
    "    \n",
    "    for fold, (train1, test1) in enumerate(kf.split(X)):\n",
    "        train=X[train1]\n",
    "        test=X[test1]\n",
    "        train=pd.DataFrame(data=train, columns=col_names)\n",
    "        test=pd.DataFrame(data=test, columns=col_names)\n",
    "        \n",
    "        # Store test indices to identify compounds\n",
    "        test_indices = test['original_index'].values\n",
    "        \n",
    "        X_train = train[['MW','Volume','G_sol','DeltaG_sol','sol_dip',\n",
    "                     'LsoluHsolv','LsolvHsolu','SASA','O_charges',\n",
    "                     'C_charges','Most_neg','Most_pos','Het_charges']]\n",
    "        y_train = train['LogS']\n",
    "        X_test = test[['MW','Volume','G_sol','DeltaG_sol','sol_dip',\n",
    "                     'LsoluHsolv','LsolvHsolu','SASA','O_charges',\n",
    "                     'C_charges','Most_neg','Most_pos','Het_charges']]\n",
    "        y_test = test['LogS']\n",
    "        \n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        y_train=[[i] for i in y_train]\n",
    "        y_train=np.array(y_train)\n",
    "        \n",
    "        #run models\n",
    "        kernel = GPy.kern.RBF(input_dim=13, variance=1., lengthscale=1.)\n",
    "        GPR=GPy.models.GPRegression(X_train,y_train,kernel)\n",
    "        GPR.optimize()\n",
    "        gpr2preds = GPR.predict(X_test)[0]\n",
    "        errors=GPR.predict_quantiles(X_test,quantiles=(16,84)) # 1 SD confidence interval\n",
    "        \n",
    "        gpr2preds=[i[0] for i in gpr2preds]\n",
    "        errors[0]=[i[0] for i in errors[0]]\n",
    "        errors[1]=[i[0] for i in errors[1]]\n",
    "        \n",
    "        # Store predictions for this fold\n",
    "        for i in range(len(test_indices)):\n",
    "            all_predictions.append({\n",
    "                'original_index': test_indices[i],\n",
    "                'fold': fold+1,\n",
    "                'actual': y_test.iloc[i],\n",
    "                'predicted': gpr2preds[i],\n",
    "                'lower_bound': errors[0][i],\n",
    "                'upper_bound': errors[1][i]\n",
    "            })\n",
    "        \n",
    "        #evaluate model\n",
    "        GPR_R2.append(pearsonr(gpr2preds, y_test))\n",
    "        GPR_RMSE.append(rmse(gpr2preds, y_test))\n",
    "        GPR_N1.append(within_range(y_test,gpr2preds,1))\n",
    "        GPR_N05.append(within_range(y_test,gpr2preds,0.7))\n",
    "        GPR_N1_e.append(within_range_errors(y_test,gpr2preds,errors[0],errors[1],1))\n",
    "        GPR_N05_e.append(within_range_errors(y_test,gpr2preds,errors[0],errors[1],0.7))\n",
    "    \n",
    "    # Convert predictions to DataFrame\n",
    "    predictions_df = pd.DataFrame(all_predictions)\n",
    "    \n",
    "    #get R2 from Pearson output\n",
    "    GPR_R2=get_R2(GPR_R2)\n",
    "    #get mean metrics and put together in lists\n",
    "    R2.append(statistics.mean(GPR_R2))\n",
    "    RMSE.append(statistics.mean(GPR_RMSE))\n",
    "    N1.append(statistics.mean(GPR_N1))\n",
    "    N05.append(statistics.mean(GPR_N05))\n",
    "    N1_e.append(statistics.mean(GPR_N1))\n",
    "    N05_e.append(statistics.mean(GPR_N05))\n",
    "    \n",
    "    #create dataframe of metrics\n",
    "    Models=[\"GPR\"]\n",
    "    Metrics=list(zip(Models,R2,RMSE,N1,N05,N1_e,N05_e))\n",
    "    Metrics_df=pd.DataFrame(data=Metrics, columns=['Model','R2','RMSE','% within 1','% within 0.7','Max % within 1','Max % within 0.7'])\n",
    "    \n",
    "    return Metrics_df, predictions_df\n",
    "\n",
    "##method to put it all together\n",
    "def get_CV_metrics(Dataset, output_metrics, output_predictions):\n",
    "    ##get metrics and predictions\n",
    "    CV_metrics_df, predictions_df = CV_metrics(Dataset, 10)  ##10-folds\n",
    "    \n",
    "    ##save to files\n",
    "    CV_metrics_df.to_csv(output_metrics, index=False)\n",
    "    predictions_df.to_csv(output_predictions, index=False)\n",
    "\n",
    "##section 4: run CV method and get metrics and predictions\n",
    "get_CV_metrics(Dataset, output_metrics, output_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
