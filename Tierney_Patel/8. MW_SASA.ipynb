{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MW              solute_inchikey\n",
      "0  312.453  WWYNJERNGUHSAO-XUDSTZEENA-N\n",
      "1  312.453  WWYNJERNGUHSAO-XUDSTZEENA-N\n",
      "2  249.310  BZZFPIRKNVWTKJ-UHFFFAOYNA-N\n",
      "3  193.224  HQKJYAGNQSMRJK-FZOZFQFYNA-N\n",
      "4  380.784  BUYMVQAILCEWRR-UHFFFAOYNA-N\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/Users/stella/Downloads/tierney/project/original datasets/cross_solvent_solubility_dataset_BSc_2024.csv')\n",
    "\n",
    "# Extract the desired columns\n",
    "extracted_data = df[['MW', 'solute_inchikey']]\n",
    "\n",
    "# If you want to save the extracted data to a new CSV file\n",
    "extracted_data.to_csv('mw_data.csv', index=False)\n",
    "\n",
    "# Display the extracted data (optional)\n",
    "print(extracted_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 8562\n",
      "Unique rows: 3268\n",
      "Duplicates removed: 5294\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('mw_data.csv')\n",
    "\n",
    "# Drop duplicates based on solute_inchikey, keeping the first occurrence\n",
    "df_unique = df.drop_duplicates(subset=['solute_inchikey'], keep='first')\n",
    "df_unique.to_csv('MW_data.csv', index=False)\n",
    "\n",
    "\n",
    "# Print the number of duplicates removed\n",
    "print(f\"Original rows: {len(df)}\")\n",
    "print(f\"Unique rows: {len(df_unique)}\")\n",
    "print(f\"Duplicates removed: {len(df) - len(df_unique)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit\n",
      "  Downloading rdkit-2024.9.6-cp313-cp313-macosx_10_15_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rdkit) (2.2.4)\n",
      "Collecting Pillow (from rdkit)\n",
      "  Downloading pillow-11.1.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (9.1 kB)\n",
      "Downloading rdkit-2024.9.6-cp313-cp313-macosx_10_15_x86_64.whl (30.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.0/30.0 MB\u001b[0m \u001b[31m758.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp313-cp313-macosx_10_13_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m273.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Pillow, rdkit\n",
      "Successfully installed Pillow-11.1.0 rdkit-2024.9.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/7\n",
      "Processed batch 2/7\n",
      "Processed batch 3/7\n",
      "Processed batch 4/7\n",
      "Processed batch 5/7\n",
      "Processed batch 6/7\n",
      "Processed batch 7/7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import mmap  \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_sol_file(filename, sol_path):\n",
    "    \"\"\"Process a single .aux file with mmap for fast reads\"\"\"\n",
    "    try:\n",
    "        with open(os.path.join(sol_path, filename), 'rb') as f:\n",
    "            # Use mmap for zero-copy file access\n",
    "            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped_file:\n",
    "                # Binary regex for performance\n",
    "                mw_match = re.search(br'MOLECULAR_WEIGHT:AMU=([+-]?\\d+\\.\\d+D[+-]?\\d{2})', mmapped_file)\n",
    "                sasa_match = re.search(br'AREA:SQUARE ANGSTROMS=([+-]?\\d+\\.\\d+D[+-]?\\d{2})', mmapped_file)\n",
    "                \n",
    "                if mw_match and sasa_match:\n",
    "                    return {\n",
    "                        \"InChIkey\": filename.split('_')[1],\n",
    "                        \"Solvent\": filename.split('_')[2].replace('.aux', ''),\n",
    "                        \"MW\": float(mw_match.group(1).replace(b'D', b'E')),\n",
    "                        \"SASA\": float(sasa_match.group(1).replace(b'D', b'E'))\n",
    "                    }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {filename}: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "def extract_sol_data(sol_path, output_csv, batch_size=500, max_workers=8):\n",
    "    \"\"\"Process files in parallel batches\"\"\"\n",
    "    files = [f for f in os.listdir(sol_path) if f.startswith('PM6_') and f.endswith('.aux')]\n",
    "    \n",
    "    # Write CSV header\n",
    "    pd.DataFrame(columns=[\"InChIkey\", \"Solvent\", \"MW\", \"SASA\"]).to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Process in chunks\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch = files[i:i + batch_size]\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = list(executor.map(lambda f: process_sol_file(f, sol_path), batch))\n",
    "        \n",
    "        # Append results incrementally\n",
    "        pd.DataFrame([r for r in results if r]).to_csv(\n",
    "            output_csv, mode='a', header=False, index=False\n",
    "        )\n",
    "        print(f\"Processed batch {i//batch_size + 1}/{(len(files)//batch_size)+1}\")\n",
    "\n",
    "# Run it\n",
    "sol_path = '/Users/stella/Documents/aux_files'\n",
    "extract_sol_data(sol_path, 'sasa_mw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final merged shape: (3249, 15)\n",
      "Unique InChIkeys: 283\n",
      "Unique Solvents: 84\n",
      "\n",
      "Sample data:\n",
      "                      InChIkey      Solvent       MW     SASA      G_sol  \\\n",
      "0  GVEPBJHOBDJJJI-UHFFFAOYNA-N   chloroform  202.255  227.244 -23009.264   \n",
      "1  GVIJJXMXTUZIOD-UHFFFAOYNA-N  1,4-dioxane  216.315  223.495 -23380.074   \n",
      "2  WBYWAXJHAXSJNI-KZFATGLANA-N      acetone  148.161  181.325 -22781.904   \n",
      "\n",
      "   DeltaG_sol   volume   sol_dip  Lsolu_Hsolv  Lsolv_Hsolu  O_charges  \\\n",
      "0 -445.344204  243.071  0.325051        9.985        8.058    0.00000   \n",
      "1 -381.436316  239.592  1.712100        9.121       10.254    0.00000   \n",
      "2 -522.368375  182.608  2.469010        9.490       10.372   -1.10051   \n",
      "\n",
      "   C_charges  Most_neg  Most_pos  Het_charges  \n",
      "0   -1.61070  -0.15618   0.16410      0.00000  \n",
      "1   -1.27891  -0.13958   0.16914     -0.03833  \n",
      "2   -0.44112  -0.60299   0.68712     -1.10051  \n",
      "\n",
      "Saved 3249 rows to /Users/stella/Downloads/tierney/project/all_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load data\n",
    "df_sasa = pd.read_csv('/Users/stella/Downloads/tierney/project/sasa_mw_data.csv')\n",
    "df_all = pd.read_csv('/Users/stella/Downloads/tierney/project/total_data.csv')\n",
    "\n",
    "# 2. Verify column names exist\n",
    "required_cols = ['InChIkey', 'Solvent']  # or ['solute_inchikey', 'solvent']?\n",
    "for col in required_cols:\n",
    "    if col not in df_sasa.columns:\n",
    "        raise ValueError(f\"sasa_mw_data.csv missing column: {col}\")\n",
    "    if col not in df_all.columns:\n",
    "        raise ValueError(f\"total_data.csv missing column: {col}\")\n",
    "\n",
    "# 3. Perform the merge\n",
    "df_merge = df_sasa.merge(df_all, on=required_cols, how='inner')\n",
    "\n",
    "# 4. Verify output\n",
    "print(f\"\\nFinal merged shape: {df_merge.shape}\")\n",
    "print(\"Unique InChIkeys:\", df_merge['InChIkey'].nunique())\n",
    "print(\"Unique Solvents:\", df_merge['Solvent'].nunique())\n",
    "print(\"\\nSample data:\")\n",
    "print(df_merge.head(3))\n",
    "\n",
    "# 5. Save results\n",
    "output_path = '/Users/stella/Downloads/tierney/project/all_data.csv'\n",
    "df_merge.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved {len(df_merge)} rows to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
